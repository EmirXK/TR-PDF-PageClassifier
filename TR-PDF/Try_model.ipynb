{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORT MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Define the paths\n",
    "model_path = 'model2.keras'\n",
    "tokenizer_path = 'tokenizer.pickle'\n",
    "label_encoder_path = 'label_encoder.pickle'\n",
    "\n",
    "model = load_model('model2.keras')\n",
    "\n",
    "# Load the tokenizer\n",
    "with open(tokenizer_path, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Load the label encoder\n",
    "with open(label_encoder_path, 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test Model(CSV FILE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation results - Loss: 0.05521168187260628, Accuracy: 0.9900000095367432\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step\n",
      "Predictions added to the CSV file successfully.\n",
      "Matches:  4101 / 4167\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "csv_path = 'prediction_data.csv'\n",
    "output_csv_path = 'prediction_data_with_predictions_test.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure all entries in the \"Text\" column are strings and handle missing values\n",
    "data['Text'] = data['Text'].astype(str).fillna('')\n",
    "\n",
    "# Extract the \"Text\" column into a list\n",
    "new_texts = data['Text'].tolist()\n",
    "\n",
    "# Extract Label data\n",
    "label_data = data['Label']\n",
    "\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "# Evaluate model to build the compiled metrics\n",
    "# Use a small batch of data to avoid computational overhead if necessary\n",
    "eval_data = new_padded_sequences[:100]  # Adjust size as needed\n",
    "eval_labels = label_data[:100].values  # Adjust size as needed\n",
    "loss, accuracy = model.evaluate(eval_data, eval_labels, verbose=0)\n",
    "print(f\"Model evaluation results - Loss: {loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict with the model\n",
    "predictions = tf.nn.sigmoid(model.predict(new_padded_sequences)).numpy()\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_labels.flatten())\n",
    "\n",
    "# Add the predictions to the original DataFrame\n",
    "data['prediction'] = predicted_labels\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "data.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Predictions added to the CSV file successfully.\")\n",
    "\n",
    "# Count matches\n",
    "count = 0\n",
    "for i in range(len(label_data)):\n",
    "    if int(label_data[i]) == predicted_labels[i]: count += 1\n",
    "\n",
    "print(\"Matches: \", str(count), \"/\", str(len(label_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PDF INPUT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Toplam sayfa sayısı: 232\n",
      "[6, 7, 9, 10, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 216]\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "PATH_TO_PDF = '230911.pdf'\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # List for the PDF  Text\n",
    "    text_list = []\n",
    "\n",
    "    for page_num in range(len(pdf_document)):\n",
    "\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        text_list.append(page_text)\n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return text_list\n",
    "\n",
    "new_texts = extract_text_from_pdf(PATH_TO_PDF)\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=200, padding='post', truncating='post')\n",
    "\n",
    "predictions = tf.nn.sigmoid(model.predict(new_padded_sequences)).numpy()\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_labels.flatten())\n",
    "\n",
    "output_list = []\n",
    "\n",
    "for i in range(len(predicted_labels)):\n",
    "  if predicted_labels[i] == 1: \n",
    "    output_list.append(i+1)\n",
    "    \n",
    "print(f\"Toplam sayfa sayısı: {len(predicted_labels)}\")\n",
    "print(output_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **URL INPUT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded successfully and saved as downloaded_file.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import requests\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"PDF downloaded successfully and saved as {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download PDF: {response.status_code}\")\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # List for the PDF  Text\n",
    "    text_list = []\n",
    "\n",
    "\n",
    "    for page_num in range(len(pdf_document)):\n",
    "\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        text_list.append(page_text)\n",
    "\n",
    "    pdf_document.close()\n",
    "\n",
    "    return text_list\n",
    "\n",
    "\n",
    "url = 'PDFURL'\n",
    "save_path = 'downloaded_file.pdf'\n",
    "\n",
    "download_pdf(url, save_path)\n",
    "new_texts = extract_text_from_pdf('downloaded_file.pdf') \n",
    "#Same step above\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
